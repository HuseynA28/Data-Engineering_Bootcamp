# Hadoop Setup Guide

This guide will help you set up Hadoop step by step on a single-node cluster.

---

## 1. Install Prerequisites

### Update Packages and Install Java 8 (OpenJDK)
```bash
sudo dnf -y update
sudo dnf -y install java-1.8.0-openjdk-devel
```

### Install wget and nano
```bash
sudo dnf -y install wget nano
```

### Verify Java Installation
```bash
java -version
```
You should see output similar to:
```
openjdk version "1.8.0_..."
```

---

## 2. Install and Configure SSH

### Install SSH Server
```bash
sudo dnf -y install openssh-server
```

### Enable and Start SSH Service
```bash
sudo systemctl enable sshd
sudo systemctl start sshd
```

### Check SSH Status (Optional)
```bash
systemctl status sshd
```
It should show **active (running)**.

---

## 3. Configure Password-less SSH (For User vagrant)

### Switch to vagrant user
```bash
su - vagrant
```

### Generate SSH Keys (No Passphrase)
```bash
ssh-keygen -t rsa -P ""
```
Press **Enter** at prompts to accept defaults.

### Add Key to Authorized Keys
```bash
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
chmod 640 ~/.ssh/authorized_keys
```

### Test SSH Login
```bash
ssh localhost
```
Type **yes** if prompted.
You should **not** be asked for a password.

---

## 4. Download and Extract Hadoop

### Create a Folder for Hadoop
```bash
cd ~
mkdir 02.Hadoop
cd 02.Hadoop
```

### Download Hadoop 3.3.6
```bash
wget https://dlcdn.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz
```

### Extract Hadoop Files
```bash
tar -xvzf hadoop-3.3.6.tar.gz
```

### Rename Folder (Optional)
```bash
mv hadoop-3.3.6 hadoop
```

---

## 5. Configure Environment Variables

### Edit `.bashrc`
```bash
nano ~/.bashrc
```
Add these lines at the end:
```bash
# Hadoop environment variables
export JAVA_HOME=/usr/lib/jvm/java-1.8.0
export HADOOP_HOME=~/02.Hadoop/hadoop
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib/native"
```

### Save and Exit
Press **Ctrl + X**, then **Y**, then **Enter**.

### Reload Environment Variables
```bash
source ~/.bashrc
```

---

## 6. Configure Hadoop

### Edit `hadoop-env.sh`
```bash
nano ~/02.Hadoop/hadoop/etc/hadoop/hadoop-env.sh
```
Find (or add) this line:
```bash
export JAVA_HOME=/usr/lib/jvm/java-1.8.0
```
Save and exit.

---

## 7. Create HDFS Directories
```bash
mkdir -p ~/hadoopdata/hdfs/{namenode,datanode}
```

---

## 8. Configure Hadoop XML Files

### `core-site.xml`
```bash
nano ~/02.Hadoop/hadoop/etc/hadoop/core-site.xml
```
Replace with:
```xml
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value>
    </property>
</configuration>
```
Save and exit.

### `hdfs-site.xml`
```bash
nano ~/02.Hadoop/hadoop/etc/hadoop/hdfs-site.xml
```
Replace with:
```xml
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>file:///home/vagrant/hadoopdata/hdfs/namenode</value>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>file:///home/vagrant/hadoopdata/hdfs/datanode</value>
    </property>
</configuration>
```
Save and exit.

---

## 9. Format the NameNode
```bash
hdfs namenode -format
```
If successful, you will see:
```
Storage directory /home/vagrant/hadoopdata/hdfs/namenode has been successfully formatted.
```

---

## 10. Start Hadoop Services
```bash
start-all.sh
```
(Note: On newer versions, this automatically starts both **HDFS** and **YARN**.)

### Check Running Processes
```bash
jps
```
Expected output:
```
13789 SecondaryNameNode
13987 ResourceManager
13654 NameNode
13890 NodeManager
14098 Jps
13723 DataNode
```

---

## 11. Access Hadoop Web UI

- **NameNode UI**: `http://localhost:9870`
- **ResourceManager UI**: `http://localhost:8088`

---

## 12. Basic HDFS Test

### Create Directories
```bash
hdfs dfs -mkdir /test1
hdfs dfs -mkdir /logs
```

### List Directories
```bash
hdfs dfs -ls /
```
You should see `/test1` and `/logs`.

### Upload Log Files to HDFS
```bash
hdfs dfs -put /var/log/* /logs/
```

### Verify Upload
```bash
hdfs dfs -ls /logs
```

---

## 13. Stop Hadoop Services
```bash
stop-all.sh
```

---

## ðŸŽ‰ Done!
You have successfully set up Hadoop on your single-node cluster. ðŸŽ¯

