# Snowflake Documentation

This documentation covers both the setup and data loading process as well as guidelines for using Snowflake warehouses effectively and efficiently.

---

## Table of Contents

1. [Snowflake Setup and Data Loading Guide](#snowflake-setup-and-data-loading-guide)
2. [Using Snowflake Warehouses Effectively and Efficiently](#using-snowflake-warehouses-effectively-and-efficiently)

---

## 1. Snowflake Setup and Data Loading Guide

This section explains how to set up users, roles, warehouses, file formats, and stages in Snowflake and how to load the Airbnb dataset.

### Create a User

Log in to Snowflake and create a new user named `dbt_user` with the following SQL command. This user uses the `COMPUTE_WH` warehouse and the `TRANSFORM` role by default.

```sql
CREATE USER IF NOT EXISTS dbt_user
  PASSWORD='StrongPasword'
  LOGIN_NAME='dbt'
  DEFAULT_WAREHOUSE='COMPUTE_WH'
  DEFAULT_ROLE=TRANSFORM
  MUST_CHANGE_PASSWORD=FALSE;
```

Then, grant the `TRANSFORM` role to the user:

```sql
GRANT ROLE TRANSFORM TO USER dbt_user;
```

---

### Download the Dataset

Download the Airbnb Amsterdam dataset from [Kaggle](https://www.kaggle.com/datasets/erikbruin/airbnb-amsterdam) (or use your preferred dataset source).

---

### Create a New Role

We will create a new role called `dbt_role` that will later be used in DBT (data build tool) processes. Switch to the `ACCOUNTADMIN` role and run:

```sql
USE ROLE ACCOUNTADMIN;

CREATE ROLE IF NOT EXISTS dbt_role;
GRANT ROLE dbt_role TO ROLE ACCOUNTADMIN;
```

*Note:* While customer roles are typically assigned to a system administrator, in this case—with only one customer user—we assign the role directly to `ACCOUNTADMIN`.

---

### Create and Grant Privileges on a Warehouse

Create a warehouse called `dbt_warehouse` and grant the `OPERATE` privilege to `dbt_role`.

```sql
CREATE OR REPLACE WAREHOUSE dbt_warehouse WITH
  WAREHOUSE_SIZE='X-SMALL'
  AUTO_SUSPEND = 30
  AUTO_RESUME = TRUE
  INITIALLY_SUSPENDED=TRUE;

GRANT OPERATE ON WAREHOUSE dbt_warehouse TO ROLE dbt_role;
```

#### Key Access Control Concepts

- **Securable Object:** An entity (e.g., warehouse, database, table) to which access can be granted.
- **Role:** An entity that can receive privileges; roles are assigned to users (and even to other roles, forming a hierarchy).
- **Privilege:** A specific level of access (e.g., SELECT, OPERATE) to a securable object.
- **User:** A Snowflake-recognized identity, whether for a person or a program.

For additional privileges, see the [Snowflake Security and Access Control Privileges](https://docs.snowflake.com/en/user-guide/security-access-control-privileges).

---

### Define a File Format for Data Loading

Before loading data, you must tell Snowflake how to interpret the files. Create a CSV file format for the Airbnb data:

```sql
CREATE OR REPLACE FILE FORMAT AIRBND_FILE_FORMAT
  TYPE = 'CSV'
  COMPRESSION = 'AUTO'
  FIELD_DELIMITER = ','
  PARSE_HEADER = TRUE
  FIELD_OPTIONALLY_ENCLOSED_BY = '"';
```

**Explanation:**

- **TYPE:** Specifies that the file is in CSV format.
- **COMPRESSION:** Auto-detects the compression method of the file.
- **FIELD_DELIMITER:** Character used to separate columns.
- **PARSE_HEADER:** Uses the first row as column headers if true.
- **FIELD_OPTIONALLY_ENCLOSED_BY:** Indicates that fields may be enclosed by a specific character.

For more file format options, see the [Create File Format Documentation](https://docs.snowflake.com/en/sql-reference/sql/create-file-format).

---

### Create a Stage and Upload Data

A stage in Snowflake is a landing zone for your data files before they are loaded into tables.

1. **Create the Stage:**

   ```sql
   CREATE STAGE IF NOT EXISTS AIRBNB_STAGE;
   ```

2. **Upload Data:**  
   Use the Snowflake web interface (or command-line tools) to upload all Airbnb data files to the `AIRBNB_STAGE`.

3. **Inspect the Stage:**  
   - To view stage details:
     ```sql
     DESC STAGE AIRBNB_STAGE;
     ```
   - To list the files in the stage:
     ```sql
     LIST @AIRBNB_STAGE;
     ```

---

### Infer Schema and Create a Table

Before creating a table, you can use the `infer_schema` function to examine the CSV file’s structure (for example, `reviews_details.csv`):

```sql
SELECT *
FROM TABLE(
  infer_schema(
    location => '@AIRBNB_STAGE',
    files => 'reviews_details.csv',
    file_format => 'AIRBND_FILE_FORMAT'
  )
);
```

This command helps you determine the column names and data types, so you can define the table structure accordingly.

---

## 2. Using Snowflake Warehouses Effectively and Efficiently

This section provides guidelines on optimizing warehouse usage, understanding billing, scaling policies, query design best practices, and delegating warehouse management.

### 1. Experiment and Optimize

- **Test Different Queries and Sizes:**  
  Experiment with various query types and warehouse sizes to find the best combinations for your specific needs and workload.
  
- **Don’t Worry Solely About Size:**  
  Snowflake bills per second, so you can run larger warehouses (e.g., Large, X-Large, 2X-Large) and simply suspend them when not in use.

### 2. Billing and Credit Charges

- **Billing Metrics:**  
  Warehouse charges depend on:
  - **Warehouse Size**
  - **Number of Clusters:** (if using multi-cluster warehouses)
  - **Run Time:** The length of time each cluster's compute resources are active.
  
- **Cost Examples:**
  - **Enterprise Edition:**  
    Costs $3 per hour (approximately 5 cents per minute).
  - **Small Warehouse Example:**  
    If a small warehouse uses 2 credits per hour, the cost is:  
    `2 credits/hour x 5 cents = 10 cents per minute`.
  - **Multi-Cluster Warehouse Example:**  
    For a warehouse with a minimum and maximum of 2 clusters, the cost becomes:  
    `2 clusters x 2 credits/hour x 5 cents = 20 cents per minute`.

- **Minimum Billing Time:**
  - If a warehouse runs for 30–60 seconds, you’re billed for 60 seconds.
  - If it runs for 61 seconds, you’re billed for 61 seconds.
  - Restarting a warehouse counts separately. For example, if a warehouse runs for 61 seconds, shuts down, and then runs for less than 60 seconds after restarting, the total billed time might be calculated as 60 + 1 + 60 seconds (121 seconds).

### 3. Scaling Policies

- **Scaling Policy Options:**  
  There are two primary scaling policies:
  - **Standard Scaling:**  
    Balances performance and cost for typical workloads.
  - **Economy Scaling:**  
    Focuses on reducing costs, possibly at the expense of peak performance.
  
  *Note:* Always consider the trade-off between performance improvements and potential additional costs when changing scaling policies.

### 4. Resizing Warehouses

- **Billing Implications:**  
  Resizing (especially from a very large size like 5XL or 6XL to 4XL or smaller) can temporarily incur charges for both the new and old warehouse during the transition period.
  
- **Impact on Running Queries:**
  - **Resizing Up:**  
    Additional compute clusters are added without affecting the current cache, so performance remains stable.
  - **Resizing Down:**  
    Removing clusters causes the cache associated with those clusters to be lost, which may temporarily slow down query performance.
  
- **Key Consideration:**  
  There is a trade-off between saving credits and maintaining cache performance.

### 5. Query Design Best Practices

- **Data Size vs. Row Count:**  
  The overall size of the data (in GB/TB) impacts query performance more than the number of rows.
  
- **Optimizing Joins:**
  - **Filter Early:**  
    Use the `WHERE` clause to limit data before joining tables.
  - **Minimize Unnecessary Joins:**  
    Only join tables that are essential for your query to avoid extra overhead.

### 6. Workload Homogeneity and Cache Benefits

- **Homogeneous Workloads:**  
  Running similar types of queries on the same warehouse helps analyze load and select the best warehouse size.
  
- **Cache Advantages:**  
  Each running warehouse maintains a cache of accessed data, improving performance for subsequent queries by reducing the need to scan the entire table again.

### 7. Warehouse Size Recommendations

- **Small-Scale Testing:**  
  X-Small, Small, or Medium warehouses are usually sufficient.
  
- **Large-Scale Production:**  
  Larger warehouses (Large, X-Large, 2X-Large, etc.) may be more cost effective for heavy workloads.
  
- **Snowsight Usage:**  
  Some Snowsight pages (like Task Run History or Data Preview) require a warehouse to run SQL queries. An X-Small warehouse is generally recommended for these, though larger accounts might benefit from a bigger size.

### 8. Dynamic Resizing While Running

- **On-the-Fly Resizing:**  
  Snowflake allows you to resize a warehouse while it’s running:
  - **Increasing Size:**  
    New compute clusters are added without disturbing the existing cache.
  - **Decreasing Size:**  
    Removing clusters causes the cache associated with those clusters to be lost, which might slow performance until the cache is rebuilt.
  
- **No Impact on Current Queries:**  
  Resizing affects only queued or new queries; running queries continue unaffected.

### 9. Delegating Warehouse Management

- **Default Permissions:**  
  The `ACCOUNTADMIN` role can alter, suspend, and manage all warehouses.
  
- **Delegating Permissions:**  
  To delegate warehouse management, grant the `MANAGE WAREHOUSES` privilege to a custom role. This privilege is equivalent to granting `MODIFY`, `MONITOR`, and `OPERATE` privileges on all warehouses.

#### Example Setup

1. **Create a Role for Warehouse Creation:**

   ```sql
   CREATE ROLE create_wh_role;
   GRANT CREATE WAREHOUSE ON ACCOUNT TO ROLE create_wh_role;
   GRANT ROLE create_wh_role TO ROLE SYSADMIN;
   ```

2. **Create a Role for Managing Warehouses:**

   ```sql
   CREATE ROLE manage_wh_role;
   GRANT MANAGE WAREHOUSES ON ACCOUNT TO ROLE manage_wh_role;
   GRANT ROLE manage_wh_role TO ROLE SYSADMIN;
   ```

3. **Using the `create_wh_role` to Create a Warehouse:**

   ```sql
   USE ROLE create_wh_role;
   CREATE OR REPLACE WAREHOUSE test_wh WITH WAREHOUSE_SIZE = XSMALL;
   ```

4. **Using the `manage_wh_role` to Manage the Warehouse:**

   ```sql
   USE ROLE manage_wh_role;
   ALTER WAREHOUSE test_wh SUSPEND;
   ALTER WAREHOUSE test_wh RESUME;
   ALTER WAREHOUSE test_wh SET WAREHOUSE_SIZE = SMALL;
   DESC WAREHOUSE test_wh;
   ```

### 10. Using the Query Acceleration Service

- **Ideal Use Cases:**
  - Ad hoc analytics.
  - Workloads with unpredictable data volumes.
  - Queries that involve large scans with selective filters.
  
- **Scale Factor Adjustment:**  
  The scale factor limits the maximum compute resources a warehouse can lease for query acceleration. For example, setting a scale factor of 5 on a medium warehouse (costing 4 credits per hour) can lead to an additional cost of up to 20 credits per hour.  
  *Note:* The extra cost applies regardless of the number of queries using the service.

### 11. Final Notes on Cost and Data Transfer

- **Minimum Billing Increment:**  
  Starting and stopping a warehouse will incur a minimum charge of one minute, even if it runs for less.
  
- **Data Transfer Costs:**  
  Transferring data from Snowflake to local environments may incur additional costs, which are determined by your cloud provider (AWS, Azure, or GCP).

---

This documentation serves as a comprehensive guide for setting up and managing Snowflake resources, as well as for optimizing warehouse performance and cost efficiency. Use it as a reference to configure your environment and design efficient queries.

