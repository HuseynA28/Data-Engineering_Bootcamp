# Snowflake Setup and Data Loading Guide

This guide will help you set up users, roles, warehouses, file formats, and stages in Snowflake, load the Airbnb dataset, and understand some key Snowflake concepts like micro-partitions and access control.

---

## 1. Create a User

Log in to Snowflake and create a new user named `dbt_user` with the following SQL command. This user uses the `COMPUTE_WH` warehouse and the `TRANSFORM` role by default.

```sql
CREATE USER IF NOT EXISTS dbt_user
  PASSWORD='StrongPasword'
  LOGIN_NAME='dbt'
  DEFAULT_WAREHOUSE='COMPUTE_WH'
  DEFAULT_ROLE=TRANSFORM
  MUST_CHANGE_PASSWORD=FALSE;
```

Then, grant the `TRANSFORM` role to the user:

```sql
GRANT ROLE TRANSFORM TO USER dbt_user;
```

---

## 2. Download the Dataset

Download the Airbnb Amsterdam dataset from [Kaggle](https://www.kaggle.com/datasets/erikbruin/airbnb-amsterdam) (or use your preferred dataset source).

---

## 3. Create a New Role

We will create a new role called `dbt_role` that will later be used in DBT (data build tool) processes. Switch to the `ACCOUNTADMIN` role and run:

```sql
USE ROLE ACCOUNTADMIN;

CREATE ROLE IF NOT EXISTS dbt_role;
GRANT ROLE dbt_role TO ROLE ACCOUNTADMIN;
```

*Note:* While customer roles are typically assigned to a system administrator, in this case—with only one customer user—we assign the role directly to `ACCOUNTADMIN`.

---

## 4. Create and Grant Privileges on a Warehouse

Create a warehouse called `dbt_warehouse` and grant the `OPERATE` privilege to `dbt_role`.

```sql
CREATE OR REPLACE WAREHOUSE dbt_warehouse WITH
  WAREHOUSE_SIZE='X-SMALL'
  AUTO_SUSPEND = 30
  AUTO_RESUME = TRUE
  INITIALLY_SUSPENDED=TRUE;

GRANT OPERATE ON WAREHOUSE dbt_warehouse TO ROLE dbt_role;
```

### Key Access Control Concepts

- **Securable Object:** An entity (e.g., warehouse, database, table) to which access can be granted.
- **Role:** An entity that can receive privileges; roles are assigned to users (and even to other roles, forming a hierarchy).
- **Privilege:** A specific level of access (e.g., SELECT, OPERATE) to a securable object.
- **User:** A Snowflake-recognized identity, whether for a person or a program.

For additional privileges, see the [Snowflake Security and Access Control Privileges](https://docs.snowflake.com/en/user-guide/security-access-control-privileges).

---

## 5. Define a File Format for Data Loading

Before loading data, you must tell Snowflake how to interpret the files. Create a CSV file format for the Airbnb data:

```sql
CREATE OR REPLACE FILE FORMAT AIRBND_FILE_FORMAT
  TYPE = 'CSV'
  COMPRESSION = 'AUTO'
  FIELD_DELIMITER = ','
  PARSE_HEADER = TRUE
  FIELD_OPTIONALLY_ENCLOSED_BY = '"';
```

**Explanation:**
- **TYPE:** Specifies that the file is in CSV format.
- **COMPRESSION:** Auto-detects the compression method of the file.
- **FIELD_DELIMITER:** Character used to separate columns.
- **PARSE_HEADER:** Uses the first row as column headers if true.
- **FIELD_OPTIONALLY_ENCLOSED_BY:** Indicates that fields may be enclosed by a specific character.

For more file format options, see the [Create File Format Documentation](https://docs.snowflake.com/en/sql-reference/sql/create-file-format).

---

## 6. Create a Stage and Upload Data

A stage in Snowflake is a landing zone for your data files before they are loaded into tables.

1. **Create the Stage:**

   ```sql
   CREATE STAGE IF NOT EXISTS AIRBNB_STAGE;
   ```

2. **Upload Data:**
   - Use the Snowflake web interface (or command-line tools) to upload all Airbnb data files to the `AIRBNB_STAGE`.

3. **Inspect the Stage:**
   - To view stage details:
   
     ```sql
     DESC STAGE AIRBNB_STAGE;
     ```
     
   - To list the files in the stage:
   
     ```sql
     LIST @AIRBNB_STAGE;
     ```

---

## 7. Infer Schema and Create a Table

Before creating a table, you can use the `infer_schema` function to examine the CSV file’s structure (e.g., `reviews_details.csv`):

```sql
SELECT *
FROM TABLE(
  infer_schema(
    location => '@AIRBNB_STAGE',
    files => 'reviews_details.csv',
    file_format => 'AIRBND_FILE_FORMAT'
  )
);
```

This command helps you determine the column names and data types, so you can define the table structure accordingly.

---

## 8. Understanding Snowflake Table Structures and Micro-Partitions

### Snowflake Table Structures
- All data in Snowflake is stored in tables that consist of rows and columns.
- Internally, data is automatically divided into **micro-partitions**—small, contiguous storage units (50 MB to 500 MB before compression).

### Micro-Partitions
- **Metadata Storage:** Each micro-partition stores metadata (e.g., the range of column values, distinct counts) to help optimize queries.
- **Pruning:** Snowflake uses this metadata to skip scanning micro-partitions that do not match a query’s filter criteria.

*Important Note:*
- **Predicate Limitations:** Not all filter conditions (predicates) are eligible for pruning. For example, if a predicate contains a subquery—even if that subquery returns a constant—Snowflake cannot use it for pruning, which might result in scanning more data than necessary.

*Example:*
- A straightforward predicate like:
  
  ```sql
  WHERE order_date > '2024-01-01'
  ```
  
  allows Snowflake to prune micro-partitions based on the metadata.
  
- A predicate with a subquery:
  
  ```sql
  WHERE order_date > (SELECT '2024-01-01')
  ```
  
  does not enable pruning, potentially affecting query performance.

---

The table consists of 24 rows stored across 4 micro-partitions, with the rows divided equally between each micro-partition. Within each micro-partition, the data is sorted and stored by column, which enables Snowflake to perform the following actions for queries on the table:

First, prune micro-partitions that are not needed for the query.

Then, prune by column within the remaining micro-partitions.

Clustering Depth
The clustering depth for a populated table measures the average depth (1 or greater) of the overlapping micro-partitions for specified columns in a table. The smaller the average depth, the better clustered the table is with regards to the specified columns.

Snowflake_documentation\ClusteringDepth.png  ( this is image do not deleted  that )

Benefits of Defining Clustering Keys (for Very Large Tables)
Using a clustering key to co-locate similar rows in the same micro-partitions enables several benefits for very large tables, including:

Improved scan efficiency in queries by skipping data that does not match filtering predicates.

Better column compression than in tables with no clustering. This is especially true when other columns are strongly correlated with the columns that comprise the clustering key.

After a key has been defined on a table, no additional administration is required, unless you chose to drop or modify the key. All future maintenance on the rows in the table (to ensure optimal clustering) is performed automatically by Snowflake.

Typically, queries benefit from clustering when the queries filter or sort on the clustering key for the table. Sorting is commonly done for ORDER BY operations, for GROUP BY operations, and for some joins. For example, the following join would likely cause Snowflake to perform a sort operation:

SELECT ...
    FROM my_table INNER JOIN my_materialized_view
        ON my_materialized_view.col1 = my_table.col1
    ...

    In this pseudo-example, Snowflake is likely to sort the values in either my_materialized_view.col1 or my_table.col1. For example, if the values in my_table.col1 are sorted, then as the materialized view is being scanned, Snowflake can quickly find the corresponding row in my_table.



Breaking it down:
Materialized Views:

A materialized view is like a snapshot of a query result that is stored and automatically updated when the underlying data changes. It's designed to make queries faster because Snowflake doesn’t have to compute the results from scratch each time.
Sorting in my_materialized_view.col1 or my_table.col1:

The sentence suggests that Snowflake might sort the values in either:
The materialized view's column (my_materialized_view.col1), or
The original table's column (my_table.col1).
Why sorting matters: When data is sorted, finding specific rows becomes faster because Snowflake can quickly locate the data without scanning everything. It’s similar to finding a word in a dictionary—the words are sorted alphabetically, making it quicker to find what you're looking for.
How it helps:

If my_table.col1 is sorted, Snowflake can quickly match rows from the materialized view to the original table when executing queries.
This process helps improve query speed because it avoids full table scans and leverages the sorted order to jump directly to the needed data.
In simple terms:
Imagine you have two lists of names:

List 1: A snapshot of names (materialized view).
List 2: The original list of names (my_table).
If both lists are sorted alphabetically, you can quickly match names between them without having to check every single one. That’s what Snowflake is doing here—sorting the data so it can quickly find and match rows, which speeds up your queries.




The more frequently a table is queried, the more benefit clustering provides. However, the more frequently a table changes, the more expensive it will be to keep it clustered. Therefore, clustering is generally most cost-effective for tables that are queried frequently and do not change frequently.