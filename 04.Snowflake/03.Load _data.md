# File Formats
CSV 
JSON 
XML
PARQUET
ORC
AVRO






# Snowflake Setup and Data Loading Guide

This guide will help you set up users, roles, warehouses, file formats, and stages in Snowflake, load the Airbnb dataset, and understand some key Snowflake concepts like micro-partitions and access control.

---

## 1. Create a User

Log in to Snowflake and create a new user named `dbt_user` with the following SQL command. This user uses the `COMPUTE_WH` warehouse and the `TRANSFORM` role by default.

```sql
CREATE USER IF NOT EXISTS dbt_user
  PASSWORD='StrongPasword'
  LOGIN_NAME='dbt'
  DEFAULT_WAREHOUSE='COMPUTE_WH'
  DEFAULT_ROLE=TRANSFORM
  MUST_CHANGE_PASSWORD=FALSE;
```

Then, grant the `TRANSFORM` role to the user:

```sql
GRANT ROLE TRANSFORM TO USER dbt_user;
```

---

## 2. Download the Dataset

Download the Airbnb Amsterdam dataset from [Kaggle](https://www.kaggle.com/datasets/erikbruin/airbnb-amsterdam) (or use your preferred dataset source).

---

## 3. Create a New Role

We will create a new role called `dbt_role` that will later be used in DBT (data build tool) processes. Switch to the `ACCOUNTADMIN` role and run:

```sql
USE ROLE ACCOUNTADMIN;

CREATE ROLE IF NOT EXISTS dbt_role;
GRANT ROLE dbt_role TO ROLE ACCOUNTADMIN;
```

*Note:* While customer roles are typically assigned to a system administrator, in this case—with only one customer user—we assign the role directly to `ACCOUNTADMIN`.

---

## 4. Create and Grant Privileges on a Warehouse

Create a warehouse called `dbt_warehouse` and grant the `OPERATE` privilege to `dbt_role`.

```sql
CREATE OR REPLACE WAREHOUSE dbt_warehouse WITH
  WAREHOUSE_SIZE='X-SMALL'
  AUTO_SUSPEND = 30
  AUTO_RESUME = TRUE
  INITIALLY_SUSPENDED=TRUE;

GRANT OPERATE ON WAREHOUSE dbt_warehouse TO ROLE dbt_role;
```

### Key Access Control Concepts

- **Securable Object:** An entity (e.g., warehouse, database, table) to which access can be granted.
- **Role:** An entity that can receive privileges; roles are assigned to users (and even to other roles, forming a hierarchy).
- **Privilege:** A specific level of access (e.g., SELECT, OPERATE) to a securable object.
- **User:** A Snowflake-recognized identity, whether for a person or a program.

For additional privileges, see the [Snowflake Security and Access Control Privileges](https://docs.snowflake.com/en/user-guide/security-access-control-privileges).

---

## 5. Define a File Format for Data Loading

Before loading data, you must tell Snowflake how to interpret the files. Create a CSV file format for the Airbnb data:

```sql
CREATE OR REPLACE FILE FORMAT AIRBND_FILE_FORMAT
  TYPE = 'CSV'
  COMPRESSION = 'AUTO'
  FIELD_DELIMITER = ','
  PARSE_HEADER = TRUE
  FIELD_OPTIONALLY_ENCLOSED_BY = '"';
```

**Explanation:**
- **TYPE:** Specifies that the file is in CSV format.
- **COMPRESSION:** Auto-detects the compression method of the file.
- **FIELD_DELIMITER:** Character used to separate columns.
- **PARSE_HEADER:** Uses the first row as column headers if true.
- **FIELD_OPTIONALLY_ENCLOSED_BY:** Indicates that fields may be enclosed by a specific character.

For more file format options, see the [Create File Format Documentation](https://docs.snowflake.com/en/sql-reference/sql/create-file-format).

---cd

## 6. Create a Stage and Upload Data

A stage in Snowflake is a landing zone for your data files before they are loaded into tables.

1. **Create the Stage:**

   ```sql
   CREATE STAGE IF NOT EXISTS AIRBNB_STAGE;
   ```

2. **Upload Data:**
   - Use the Snowflake web interface (or command-line tools) to upload all Airbnb data files to the `AIRBNB_STAGE`.

3. **Inspect the Stage:**
   - To view stage details:
   
     ```sql
     DESC STAGE AIRBNB_STAGE;
     ```
     
   - To list the files in the stage:
   
     ```sql
     LIST @AIRBNB_STAGE;
     ```

---

## 7. Infer Schema and Create a Table

Before creating a table, you can use the `infer_schema` function to examine the CSV file’s structure (e.g., `reviews_details.csv`):

```sql
SELECT *
FROM TABLE(
  infer_schema(
    location => '@AIRBNB_STAGE',
    files => 'reviews_details.csv',
    file_format => 'AIRBND_FILE_FORMAT'
  )
);
```

This command helps you determine the column names and data types, so you can define the table structure accordingly.

---

## 8. Understanding Snowflake Table Structures and Micro-Partitions

### Snowflake Table Structures
- All data in Snowflake is stored in tables that consist of rows and columns.
- Internally, data is automatically divided into **micro-partitions**—small, contiguous storage units (50 MB to 500 MB before compression).

### Micro-Partitions
- **Metadata Storage:** Each micro-partition stores metadata (e.g., the range of column values, distinct counts) to help optimize queries.
- **Pruning:** Snowflake uses this metadata to skip scanning micro-partitions that do not match a query’s filter criteria.

*Important Note:*
- **Predicate Limitations:** Not all filter conditions (predicates) are eligible for pruning. For example, if a predicate contains a subquery—even if that subquery returns a constant—Snowflake cannot use it for pruning, which might result in scanning more data than necessary.

*Example:*
- A straightforward predicate like:
  
  ```sql
  WHERE order_date > '2024-01-01'
  ```
  
  allows Snowflake to prune micro-partitions based on the metadata.
  
- A predicate with a subquery:
  
  ```sql
  WHERE order_date > (SELECT '2024-01-01')
  ```
  
  does not enable pruning, potentially affecting query performance.

---

Create a Transient table and loading into a  data REVIEWS_DETAILS  data ( We will talk about  more in netx seccsion about the table )


CREATE OR REPLACE TEMPORARY TABLE REVIEWS_DETAILS (
    listing_id   INTEGER,
    id           INTEGER,
    review_date  TIMESTAMP,
    reviewer_id  STRING,
    reviewer_name STRING,
    comments     STRING
);
COPY INTO REVIEWS_DETAILS (
                     listing_id ,
                     id ,
                     review_date ,
                     reviewer_id ,
                     reviewer_name ,
                     comments 
                        )
        FROM @AIRBNB_STAGE
        FILES = ('reviews_details.csv')
        FILE_FORMAT = (FORMAT_NAME = 'AIRBND_FILE_FORMAT_COPY_INTO_TABLE');


The table consists of 24 rows stored across 4 micro-partitions, with the rows divided equally between each micro-partition. Within each micro-partition, the data is sorted and stored by column, which enables Snowflake to perform the following actions for queries on the table:

First, prune micro-partitions that are not needed for the query.

Then, prune by column within the remaining micro-partitions.

Clustering Depth
The clustering depth for a populated table measures the average depth (1 or greater) of the overlapping micro-partitions for specified columns in a table. The smaller the average depth, the better clustered the table is with regards to the specified columns.

Snowflake_documentation\ClusteringDepth.png  ( this is image do not deleted  that )

Benefits of Defining Clustering Keys (for Very Large Tables)
Using a clustering key to co-locate similar rows in the same micro-partitions enables several benefits for very large tables, including:

Improved scan efficiency in queries by skipping data that does not match filtering predicates.

Better column compression than in tables with no clustering. This is especially true when other columns are strongly correlated with the columns that comprise the clustering key.

After a key has been defined on a table, no additional administration is required, unless you chose to drop or modify the key. All future maintenance on the rows in the table (to ensure optimal clustering) is performed automatically by Snowflake.

Typically, queries benefit from clustering when the queries filter or sort on the clustering key for the table. Sorting is commonly done for ORDER BY operations, for GROUP BY operations, and for some joins. For example, the following join would likely cause Snowflake to perform a sort operation:

SELECT ...
    FROM my_table INNER JOIN my_materialized_view
        ON my_materialized_view.col1 = my_table.col1
    ...

    In this pseudo-example, Snowflake is likely to sort the values in either my_materialized_view.col1 or my_table.col1. For example, if the values in my_table.col1 are sorted, then as the materialized view is being scanned, Snowflake can quickly find the corresponding row in my_table.



Breaking it down:
Materialized Views:

A materialized view is like a snapshot of a query result that is stored and automatically updated when the underlying data changes. It's designed to make queries faster because Snowflake doesn’t have to compute the results from scratch each time.
Sorting in my_materialized_view.col1 or my_table.col1:

The sentence suggests that Snowflake might sort the values in either:
The materialized view's column (my_materialized_view.col1), or
The original table's column (my_table.col1).
Why sorting matters: When data is sorted, finding specific rows becomes faster because Snowflake can quickly locate the data without scanning everything. It’s similar to finding a word in a dictionary—the words are sorted alphabetically, making it quicker to find what you're looking for.
How it helps:

If my_table.col1 is sorted, Snowflake can quickly match rows from the materialized view to the original table when executing queries.
This process helps improve query speed because it avoids full table scans and leverages the sorted order to jump directly to the needed data.
In simple terms:
Imagine you have two lists of names:

List 1: A snapshot of names (materialized view).
List 2: The original list of names (my_table).
If both lists are sorted alphabetically, you can quickly match names between them without having to check every single one. That’s what Snowflake is doing here—sorting the data so it can quickly find and match rows, which speeds up your queries.




The more frequently a table is queried, the more benefit clustering provides. However, the more frequently a table changes, the more expensive it will be to keep it clustered. Therefore, clustering is generally most cost-effective for tables that are queried frequently and do not change frequently.

Strategies for Selecting Clustering Keys

A single clustering key can contain one or more columns or expressions. For most tables, Snowflake recommends a maximum of 3 or 4 columns (or expressions) per key. Adding more than 3-4 columns tends to increase costs more than benefits.

Cluster columns that are most actively used in selective filters. For many fact tables involved in date-based queries (for example “WHERE invoice_date > x AND invoice date <= y”), choosing the date column is a good idea. For event tables, event type might be a good choice, if there are a large number of different event types. (If your table has only a small number of different event types, then see the comments on cardinality below before choosing an event column as a clustering key.)

If there is room for additional cluster keys, then consider columns frequently used in join predicates, for example “FROM table1 JOIN table2 ON table2.column_A = table1.column_B”.

The number of distinct values (i.e. cardinality) in a column/expression is a critical aspect of selecting it as a clustering key. It is important to choose a clustering key that has:

A large enough number of distinct values to enable effective pruning on the table.

A small enough number of distinct values to allow Snowflake to effectively group rows in the same micro-partitions.

A column with very low cardinality might yield only minimal pruning, such as a column named IS_NEW_CUSTOMER that contains only Boolean values. At the other extreme, a column with very high cardinality is also typically not a good candidate to use as a clustering key directly. For example, a column that contains nanosecond timestamp values would not make a good clustering key.

Breaking it down:

High Cardinality:
A column with very high cardinality has a lot of unique values. Using such a column directly as a clustering key can lead to many small, scattered clusters, which may not be efficient for querying.

Using an Expression:
Instead of clustering directly on the raw column, you apply an expression (a function) to it. This expression reduces the number of distinct values. For example, if you have a timestamp column, you might round it to the nearest hour or day. This groups many similar timestamps together into fewer unique values.

Preserving Original Ordering:
The expression should keep the same order as the original values. This means if one original value is less than another, the transformed value should reflect that same relationship. Maintaining this order is crucial because Snowflake uses the minimum and maximum values of each data partition to decide whether a partition can be skipped (pruned) during a query. If the ordering is preserved, those min and max values remain meaningful for efficient query pruning.

Example:

Imagine you have a timestamp column with many unique entries. Instead of clustering on the raw timestamp, you could cluster on DATE_TRUNC('hour', timestamp_column). This rounds each timestamp down to the start of its hour. Now, instead of thousands of unique timestamps, you might have just a few unique hourly buckets. Yet, the order is still maintained: earlier hours come before later hours. This lets Snowflake quickly skip partitions that don't fall within a queried time range.

For example, if a fact table has a TIMESTAMP column c_timestamp containing many discrete values (many more than the number of micro-partitions in the table), then a clustering key could be defined on the column by casting the values to dates instead of timestamps (e.g. to_date(c_timestamp)). This would reduce the cardinality to the total number of days, which typically produces much better pruning results.

As another example, you can truncate a number to fewer significant digits by using the TRUNC functions and a negative value for the scale (e.g. TRUNC(123456789, -5)).

If you are defining a multi-column clustering key for a table, the order in which the columns are specified in the CLUSTER BY clause is important. As a general rule, Snowflake recommends ordering the columns from lowest cardinality to highest cardinality. Putting a higher cardinality column before a lower cardinality column will generally reduce the effectiveness of clustering on the latter column.

In some cases, clustering on columns used in GROUP BY or ORDER BY clauses can be helpful. However, clustering on these columns is usually less helpful than clustering on columns that are heavily used in filter or JOIN operations. If you have some columns that are heavily used in filter/join operations and different columns that are used in ORDER BY or GROUP BY operations, then favor the columns used in the filter and join operations.

Reclustering
As DML operations (INSERT, UPDATE, DELETE, MERGE, COPY) are performed on a clustered table, the data in the table might become less clustered. Periodic/regular reclustering of the table is required to maintain optimal clustering.

Similar to all DML operations in Snowflake, reclustering consumes credits

Credit and Storage Impact of Reclustering
Similar to all DML operations in Snowflake, reclustering consumes credits. The number of credits consumed depends on the size of the table and the amount of data that needs to be reclustered.

This process can create significant data turnover because the original micro-partitions are marked as deleted, but retained in the system to enable Time Travel and Fail-safe. The original micro-partitions are purged only after both the Time Travel retention period and the subsequent Fail-safe period have passed (i.e. minimum of 8 days and up to 97 days for extended Time Travel, if you are using Snowflake Enterprise Edition (or higher)). This typically results in increased storage costs. For more information, see Snowflake Time Travel & Fail-safe.

IF you add cluter 
ALTER TABLE REVIEWS_DETAILS
CLUSTER BY (REVIEW_DATE);
If you  wnat to add it whicl creating 

CREATE TABLE my_table (
    id NUMBER,
    REVIEW_DATE TIMESTAMP,
    data STRING
)
CLUSTER BY (event_date);

if you wan to  get information about the clsuter 

SELECT
  SYSTEM$CLUSTERING_INFORMATION('REVIEWS_DETAILS');


  If you define two or more columns/expressions as the clustering key for a table, the order has an impact on how the data is clustered in micro-partitions.

For more details, see Strategies for Selecting Clustering Keys (in this topic).

An existing clustering key is copied when a table is created using CREATE TABLE … CLONE. However, Automatic Clustering is suspended for the cloned table and must be resumed.

An existing clustering key is not supported when a table is created using CREATE TABLE … AS SELECT; however, you can define a clustering key after the table is created.

Defining a clustering key directly on top of VARIANT columns is not supported; however, you can specify a VARIANT column in a clustering key if you provide an expression consisting of the path and the target type.


Automatic Clustering
Automatic Clustering is the Snowflake service that seamlessly and continually manages all reclustering, as needed, of clustered tables.

Note that, after a clustered table is defined, reclustering does not necessarily start immediately. Snowflake only reclusters a clustered table if it will benefit from the operation.

If you want to suspend RECLUSTER

ALTER TABLE REVIEWS_DETAILS SUSPEND RECLUSTER;



Tables in snowflake 

04.Snowflake\Snowflake_documentation\snowflaketables.png

this image do not delete dthat 

What is Permanent table?
The typical, everyday database tables are the “Permanent Tables”. Snowflake’s default table type is permanent, and making one doesn’t require any extra syntax during creation. The information kept in permanent tables takes up space and gets added to the storage fees that Snowflake charges you.

In addition, it has extra features like Fail-Safe and Time-Travel that aid in data availability and recovery.


Temporary Tables
Snowflake supports creating temporary tables for storing non-permanent, transitory data (e.g. ETL data, session-specific data). Temporary tables only exist within the session in which they were created and persist only for the remainder of the session. As such, they are not visible to other users or sessions. Once the session ends, data stored in the table is purged completely from the system and, therefore, is not recoverable, either by the user who created the table or Snowflake.


Transient Tables
Snowflake supports creating transient tables that persist until explicitly dropped and are available to all users with the appropriate privileges. Transient tables are similar to permanent tables with the key difference that they do not have a Fail-safe period. As a result, transient tables are specifically designed for transitory data that needs to be maintained beyond each session (in contrast to temporary tables), but does not need the same level of data protection and recovery provided by permanent tables.


Transient Databases and Schemas
Snowflake also supports creating transient databases and schemas. All tables created in a transient schema, as well as all schemas created in a transient database, are transient by definition.



Snowflake\Snowflake_documentation\table_types.png


external tables

An external table is a Snowflake feature that allows you to query data stored in an external stage as if the data were inside a table in Snowflake. The external stage is not part of Snowflake, so Snowflake does not store or manage the stage. To harden your security posture, you can configure the external stage for outbound private connectivity in order to access the external table using private connectivity.

External tables let you store (within Snowflake) certain file-level metadata, including filenames, version identifiers, and related properties. External tables can access data stored in any format that the COPY INTO <table> command supports except XML.

uerying data in an external table might be slower than querying data that you store natively in a table within Snowflake. To improve query performance, you can use a materialized view based on an external table. For optimal query performance when working with Parquet files, consider using Apache Iceberg™ tables instead.