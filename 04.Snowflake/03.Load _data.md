# Snowflake Setup and Data Loading Guide

This guide will help you set up users, roles, warehouses, file formats, and stages in Snowflake, load the Airbnb dataset, and understand some key Snowflake concepts like micro-partitions and access control.

---

## 1. Create a User

Log in to Snowflake and create a new user named `dbt_user` with the following SQL command. This user uses the `COMPUTE_WH` warehouse and the `TRANSFORM` role by default.

```sql
CREATE USER IF NOT EXISTS dbt_user
  PASSWORD='StrongPasword'
  LOGIN_NAME='dbt'
  DEFAULT_WAREHOUSE='COMPUTE_WH'
  DEFAULT_ROLE=TRANSFORM
  MUST_CHANGE_PASSWORD=FALSE;
```

Then, grant the `TRANSFORM` role to the user:

```sql
GRANT ROLE TRANSFORM TO USER dbt_user;
```

---

## 2. Download the Dataset

Download the Airbnb Amsterdam dataset from [Kaggle](https://www.kaggle.com/datasets/erikbruin/airbnb-amsterdam) (or use your preferred dataset source).

---

## 3. Create a New Role

We will create a new role called `dbt_role` that will later be used in DBT (data build tool) processes. Switch to the `ACCOUNTADMIN` role and run:

```sql
USE ROLE ACCOUNTADMIN;

CREATE ROLE IF NOT EXISTS dbt_role;
GRANT ROLE dbt_role TO ROLE ACCOUNTADMIN;
```

*Note:* While customer roles are typically assigned to a system administrator, in this case—with only one customer user—we assign the role directly to `ACCOUNTADMIN`.

---

## 4. Create and Grant Privileges on a Warehouse

Create a warehouse called `dbt_warehouse` and grant the `OPERATE` privilege to `dbt_role`.

```sql
CREATE OR REPLACE WAREHOUSE dbt_warehouse WITH
  WAREHOUSE_SIZE='X-SMALL'
  AUTO_SUSPEND = 30
  AUTO_RESUME = TRUE
  INITIALLY_SUSPENDED=TRUE;

GRANT OPERATE ON WAREHOUSE dbt_warehouse TO ROLE dbt_role;
```

### Key Access Control Concepts

- **Securable Object:** An entity (e.g., warehouse, database, table) to which access can be granted.
- **Role:** An entity that can receive privileges; roles are assigned to users (and even to other roles, forming a hierarchy).
- **Privilege:** A specific level of access (e.g., SELECT, OPERATE) to a securable object.
- **User:** A Snowflake-recognized identity, whether for a person or a program.

For additional privileges, see the [Snowflake Security and Access Control Privileges](https://docs.snowflake.com/en/user-guide/security-access-control-privileges).

---

## 5. Define a File Format for Data Loading

Before loading data, you must tell Snowflake how to interpret the files. Create a CSV file format for the Airbnb data:

```sql
CREATE OR REPLACE FILE FORMAT AIRBND_FILE_FORMAT
  TYPE = 'CSV'
  COMPRESSION = 'AUTO'
  FIELD_DELIMITER = ','
  PARSE_HEADER = TRUE
  FIELD_OPTIONALLY_ENCLOSED_BY = '"';
```

**Explanation:**
- **TYPE:** Specifies that the file is in CSV format.
- **COMPRESSION:** Auto-detects the compression method of the file.
- **FIELD_DELIMITER:** Character used to separate columns.
- **PARSE_HEADER:** Uses the first row as column headers if true.
- **FIELD_OPTIONALLY_ENCLOSED_BY:** Indicates that fields may be enclosed by a specific character.

For more file format options, see the [Create File Format Documentation](https://docs.snowflake.com/en/sql-reference/sql/create-file-format).

---

## 6. Create a Stage and Upload Data

A stage in Snowflake is a landing zone for your data files before they are loaded into tables.

1. **Create the Stage:**

   ```sql
   CREATE STAGE IF NOT EXISTS AIRBNB_STAGE;
   ```

2. **Upload Data:**
   - Use the Snowflake web interface (or command-line tools) to upload all Airbnb data files to the `AIRBNB_STAGE`.

3. **Inspect the Stage:**
   - To view stage details:
   
     ```sql
     DESC STAGE AIRBNB_STAGE;
     ```
     
   - To list the files in the stage:
   
     ```sql
     LIST @AIRBNB_STAGE;
     ```

---

## 7. Infer Schema and Create a Table

Before creating a table, you can use the `infer_schema` function to examine the CSV file’s structure (e.g., `reviews_details.csv`):

```sql
SELECT *
FROM TABLE(
  infer_schema(
    location => '@AIRBNB_STAGE',
    files => 'reviews_details.csv',
    file_format => 'AIRBND_FILE_FORMAT'
  )
);
```

This command helps you determine the column names and data types, so you can define the table structure accordingly.

---

## 8. Understanding Snowflake Table Structures and Micro-Partitions

### Snowflake Table Structures
- All data in Snowflake is stored in tables that consist of rows and columns.
- Internally, data is automatically divided into **micro-partitions**—small, contiguous storage units (50 MB to 500 MB before compression).

### Micro-Partitions
- **Metadata Storage:** Each micro-partition stores metadata (e.g., the range of column values, distinct counts) to help optimize queries.
- **Pruning:** Snowflake uses this metadata to skip scanning micro-partitions that do not match a query’s filter criteria.

*Important Note:*
- **Predicate Limitations:** Not all filter conditions (predicates) are eligible for pruning. For example, if a predicate contains a subquery—even if that subquery returns a constant—Snowflake cannot use it for pruning, which might result in scanning more data than necessary.

*Example:*
- A straightforward predicate like:
  
  ```sql
  WHERE order_date > '2024-01-01'
  ```
  
  allows Snowflake to prune micro-partitions based on the metadata.
  
- A predicate with a subquery:
  
  ```sql
  WHERE order_date > (SELECT '2024-01-01')
  ```
  
  does not enable pruning, potentially affecting query performance.

---

The table consists of 24 rows stored across 4 micro-partitions, with the rows divided equally between each micro-partition. Within each micro-partition, the data is sorted and stored by column, which enables Snowflake to perform the following actions for queries on the table:

First, prune micro-partitions that are not needed for the query.

Then, prune by column within the remaining micro-partitions.

Clustering Depth
The clustering depth for a populated table measures the average depth (1 or greater) of the overlapping micro-partitions for specified columns in a table. The smaller the average depth, the better clustered the table is with regards to the specified columns.

Snowflake_documentation\ClusteringDepth.png  ( this is image do not deleted  that )